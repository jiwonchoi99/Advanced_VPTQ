# ğŸ“¦ ê° Directory ì„¤ëª…

## 01_outputs
directory where the results of quantization are saved

### Save í˜•ì‹
[model_name] > v[vector_length]_c[number_of_centroids] > implemented_time

ì˜ˆë¥¼ ë“¤ì–´ /01_outputs/Llama-3.1-8B/v4_c4096/2026-01-28-13-30-02 ì´ë©´, Llama-3.1-8B ëª¨ë¸ì„ vector length 4ì™€ number of centroids ë¥¼ 4096 ìœ¼ë¡œ VPTQ ë¥¼ ì ìš©í•œ ëª¨ë¸ì´ê³  2026-01-28 13ì‹œ30ë¶„02ì´ˆì— ì‹¤í–‰ ì‹œì‘í–ˆë˜ íŒŒì¼ì´ë‹¤.
- logs : ê° gpu ì˜ ë¡œê·¸ íŒŒì¼
- model : Vector Quantization ì´ ì ìš©ëœ model (script ì—ì„œ save_model = True ë¡œ ë‘ì—ˆì„ë•Œë§Œ ìƒì„±ëœë‹¤)
- packed_model : index packing ì´ ì ìš©ëœ vector quantize model (script ì—ì„œ save_packed_model = True ë¡œ ë‘ì—ˆì„ë•Œë§Œ ìƒì„±ëœë‹¤)
- ppl_results.json : perplexity result of quantized model

## 02_script
- run_vptq.sh : íŠ¹ì • ëª¨ë¸ì„ VPTQ ë¡œ Vector Quantize í•  ë•Œ ì“°ì´ëŠ” script
- run_vptq_lora_finetuning.sh : VPTQ ë¡œ Vector Quantize ëœ ëª¨ë¸ì„ LoRA Finetuning ì‹œí‚¬ ë–„ ì“°ì´ëŠ” script

## 03_codes
- LoRA_Finetuning : codes for LoRA_Finetuning
- VPTQ : codes for VPTQ

---

# ğŸš€ How to implement VPTQ quantization
02_scripts -> run_vptq.sh ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì—´ì–´ì„œ ë³€ìˆ˜ë“¤ ì„¤ì •í•˜ê¸°!

## Multi-gpu ì„¸íŒ…
1. line 20 ì— ì‚¬ìš©í•  GPU number ë‘ line21 ì— ëª‡ ê°œ gpu ì‚¬ìš©í•˜ëŠ”ì§€ ì ê¸°
```
gpu=1,3,6
num_gpu=3
```

## ì£¼ìš” Quantization ë³€ìˆ˜ë“¤ ì„¤ì •
1. vector_lens (outlier_vector_length   vector_length) : í•œë²ˆì— ë¬¶ì„ ë²¡í„°ì˜ ê¸¸ì´ë¥¼ ì„¤ì •
> outlier ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ë•ŒëŠ” outlier_vector_length = -1 ë¡œ ë‘ê¸°
2. num_centroids (outlier_centroids   centroids) : í•˜ë‚˜ì˜ ì½”ë“œë¶ì´ ì €ì¥í•  centroids ê°œìˆ˜ ì„¤ì •
> outlier ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ë•ŒëŠ” outlier_centroids = -1 ë¡œ ë‘ê¸°
3. npercent (int) : ì „ì²´ weight ì—ì„œ int % ë§Œí¼ì„ outlier ë¡œ ì„¤ì •í•œë‹¤. 
4. num_red_centroids (outlier_residual_centroids   residual_centroids) : residual quantization ì— ì‚¬ìš© í•  í•˜ë‚˜ì˜ ì½”ë“œë¶ì´ ì €ì¥ í•  centroids ê°œìˆ˜ ì„¤ì •
> residual quantization ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ë• -1 -1 ë¡œ ë‘ê¸°
5. vector_quant_dim (in ë˜ëŠ” out) : VQ dimension ì •í•˜ê¸°
> in = ich ë°©í–¥ìœ¼ë¡œ ë²¡í„° ë¬¶ê¸° / out = och ë°©í–¥ìœ¼ë¡œ ë²¡í„° ë¬¶ê¸°
6. enable_transpose (bool) : ich ë°©í–¥ìœ¼ë¡œ ë²¡í„°ë¥¼ ë¬¶ìœ¼ë©´ enable_transpose = False, och ë°©í–¥ìœ¼ë¡œ ë¬¶ìœ¼ë©´ True ë¡œ ë‘ê¸°
7. bitwidth (int) : codebook quantize ë¥¼ ëª‡ ë¹„íŠ¸ë¡œ í• ì§€ ì •í•˜ê¸° (16ìœ¼ë¡œ ë‘ë©´ codebook quantization ì´ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤)

---

## ì‹¤í–‰ ì˜ˆì‹œ
- outlier ê³¼ residual vector quantization ê¸°ëŠ¥ ë„ê³ , bpv ë¥¼ 3ìœ¼ë¡œ ë‘ê¸° ìœ„í•´ vector length = 4, codebook size = 2^12, number of groups = 4 ìœ¼ë¡œ ì„¤ì •í•œ ë’¤ì—, och ë°©í–¥ìœ¼ë¡œ ë²¡í„°ë¥¼ ë¬¶ì–´ì„œ VPTQ ë¥¼ ì‹¤í–‰í•˜ê³ , codeobok quantization ì€ 8bit ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë³€ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. 
- ê·¸ë¦¬ê³  terminal ì— ./run_vptq.sh ì…ë ¥

```
v=4
c=4096 
...
--vector_lens -1 ${v} \ 
--num_centroids -1 ${c} \
...
--npercent 0 \
--num_res_centroids -1 -1 \
--group_num 4 \
...
--vector_quant_dim out \
--enable_transpose True \
--bitwidth 8 \
...
```


## ë‹¤ë¥¸ ì˜ˆì‹œ
ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¨ Table 8 ì˜ Llama3-8B 2.24bit ë¡œ ì„¤ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•˜ê¸° (2.24 bpv)
- outlier vector length = 4, vector length = 6
- number of outlier centroids = 4096, number of centroids = 4096
- npercent = 1 (outlier ëŠ” ì „ì²´ì˜ 1 percent)
- residual quantization ê¸°ëŠ¥ ë„ê¸° (num_red_centrodis ë¥¼ -1 -1 ë¡œ ì„¸íŒ…)
- number of groups = 16
- quantization dimension = och (vector_quant_dim = out & enable_transpose = True ë¡œ ë‘ê¸°)
- bitwidth = 16 (codebook quantization ì‹¤í–‰ ì•ˆí•¨)

```
v=6
c=4096 
...
--vector_lens 4 ${v} \
--num_centroids 4096 ${c} \
...
--npercent 1 \
--num_res_centroids -1 -1 \
--group_num 16 \
...
--vector_quant_dim out \
--enable_transpose True \
--bitwidth 16 \
...
```

## Temrinal ì— ë‚˜ì˜¤ëŠ” ê²°ê³¼ (ì„±ê³µì ìœ¼ë¡œ VPTQ ê°€ ì‘ë™í•  ë•Œ)
- {} <= ì¤‘ê´„í˜¸ ì•ˆì— ìˆëŠ” ë‚´ìš©ì€ HJê°€ ì ì–´ë‘” comment ì´ë‹¤. ì‹¤ì œ ì‹¤í–‰ì‹œí‚¬ë•ŒëŠ” terminal ì— ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤.
- ì•„ë˜ì²˜ëŸ¼ temrinal ì— ëœ¨ë©´ quantization ì´ ì˜ ì§„í–‰ë  ê²ƒì´ë‹¤.

```
(vptq) sslunder52@pim-gpu06:/home/sslunder52/project/Advaced_VPTQ/02_script$ ./run_vptq.sh {script íŒŒì¼ ì‹¤í–‰!}


Running


Running Command 1
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 124.44it/s]
model sequence length: 2048
exp time: 2026-02-09 14:18:30 
{script ì— ì ì–´ë‘” arguments ë“¤ì´ ì˜ëª» ë“¤ì–´ê°„ê²Œ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸°}
args: VPTQArguments(model_name='meta-llama/Llama-3.1-8B', seq_len=2048, quant_step=1, percdamp=0.01, blocksize=128, output_dir='/home/sslunder52/project/Advaced_VPTQ/01_outputs/Llama-3.1-8B/v6_c4096/2026-02-09-14-18-29', seed=0, eval=False, new_eval=True, save_model=False, save_packed_model=True, disable_actorder=False, hessian_path='/home/sslunder41/project/VPTQ/hess/Hessians-Llama-31-8B-Instruct-6144-8k', inv_hessian_path='/home/sslunder41/project/VPTQ/invhess/InvHessians-Llama-31-8B-Instruct-6144-8k', num_gpus=3, eval_nsamples=128, save_qlinear=True, absorb_perm=False, enable_residual=False, eval_mode=False, outlier_size=0)
quant_args: QuantizationArguments(vector_lens=[4, 6], num_centroids=[4096, 4096], num_res_centroids=[-1, -1], npercent=1.0, group_num=16, group_size=-1, kiter=100, ktol=1e-05, kseed=0, kmeans_mode='hessian', kmeans_alpha=0, enable_norm=False, norm_dim=1, enable_perm=True, enable_transpose=True, vector_quant_dim='out', bitwidth=8, bsize=1024)
Starting VPTQ...
model dtype: torch.bfloat16

----quantization start ...---- 2026-02-09 14:20:24
gpu 0 tasks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
gpu 1 tasks: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
gpu 2 tasks: [22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
gpu 1 {ìœ„ì— gpu 0 tasks ê°€ 1ë²ˆ gpu ì— í• ë‹¹ëœë‹¤}
gpu 3 
gpu 6
INFO - ----Quantizing on cuda:0----
INFO - ----Quantizing layer 0 ...---- 2026-02-09 05:20:46 on cuda:0 dtype torch.bfloat16
[['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
INFO - dict_keys(['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj'])
INFO - load Hessian from /home/sslunder41/project/VPTQ/hess/Hessians-Llama-31-8B-Instruct-6144-8k/0_qkv.pt
INFO - load inv Hessian from /home/sslunder41/project/VPTQ/invhess/InvHessians-Llama-31-8B-Instruct-6144-8k/0_qkv.pt
... 
```