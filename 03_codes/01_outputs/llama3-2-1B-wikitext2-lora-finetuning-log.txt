üöÄ Starting LoRA Fine-tuning with VPTQ Quantized Model
üìÅ Quantized model path: /home/sslunder52/project/vptq_transposed/01_outputs/Llama-3.1-8B/test6_PackedModel/v6_c8192_b16_och/2026-02-08-21-31-20/packed_model
üåê Multi-GPU training enabled with 2 GPUs
üì• Loading VPTQ quantized model and tokenizer...
‚úÖ Successfully loaded VPTQ quantized model from /home/sslunder52/project/vptq_transposed/01_outputs/Llama-3.1-8B/test6_PackedModel/v6_c8192_b16_och/2026-02-08-21-31-20/packed_model

[LoRA attachment summary]
- total params: 1,641,233,920
- trainable params: 41,943,040 (2.5556%)
- tuner layers found: 224
- LoRA-on-VQuantLinear layers: 224
- examples (first 15): ['base_model.model.model.layers.0.self_attn.q_proj', 'base_model.model.model.layers.0.self_attn.k_proj', 'base_model.model.model.layers.0.self_attn.v_proj', 'base_model.model.model.layers.0.self_attn.o_proj', 'base_model.model.model.layers.0.mlp.gate_proj', 'base_model.model.model.layers.0.mlp.up_proj', 'base_model.model.model.layers.0.mlp.down_proj', 'base_model.model.model.layers.1.self_attn.q_proj', 'base_model.model.model.layers.1.self_attn.k_proj', 'base_model.model.model.layers.1.self_attn.v_proj', 'base_model.model.model.layers.1.self_attn.o_proj', 'base_model.model.model.layers.1.mlp.gate_proj', 'base_model.model.model.layers.1.mlp.up_proj', 'base_model.model.model.layers.1.mlp.down_proj', 'base_model.model.model.layers.2.self_attn.q_proj']

trainable params: 41,943,040 || all params: 1,641,233,920 || trainable%: 2.5556
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.LoRA_VPTQ(
                (base_layer): VQuantLinear(
                  (outlier_centroids): Embedding(1, 32768)
                  (centroids): Embedding(16, 24576)
                )
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
üìä Loading and preprocessing dataset...
‚öôÔ∏è Setting up training configuration...
üéØ Initializing trainer...
üöÄ Starting training...
üìä Effective batch size: 32 (per-device=4, grad_accum=4, gpus=2)
üîç Running baseline evaluation before training...
üîç Running baseline evaluation before training...
